
name: Kind in podman

on:
  push:
  pull_request:
  workflow_dispatch:

jobs:
  kind-in-podman:
    name: Kind
    runs-on: ubuntu-${{ matrix.os }}${{ matrix.arch == 'arm64' && '-arm' || '' }}
    strategy:
      fail-fast: false
      matrix:
        os: [ 24.04 ]
        arch: [ arm64 ]
        style: [ rootless ]
        inner-podman-version: [ v5.5 ]
        exclude:
          - os: 22.04
            style: rootful
          - os: 22.04
            inner-podman-version: v5.4
    timeout-minutes: 15
    env:
      kind_create_opts: --config /etc/kind-cluster.yaml
    steps:
      - uses: actions/checkout@v4
      - name: Install podman 4.*
        uses: ./.github/actions/install-podman-4
        if: matrix.os == '22.04'
      - name: Set podman env
        run: echo "podman=podman" >> $GITHUB_ENV
      - name: Set sudo podman env
        run: echo "podman=sudo podman" >> $GITHUB_ENV
        if: matrix.style == 'rootful'
      - name: Enable cpu cgroup delegation
        run: |
          for i in /sys/fs/cgroup/user.slice/cgroup.subtree_control \
            /sys/fs/cgroup/user.slice/user-$(id -u).slice/cgroup.subtree_control \
            /sys/fs/cgroup/user.slice/user-$(id -u).slice/user@$(id -u).service/cgroup.subtree_control ; do \
            echo '+cpu' | sudo tee -a $i ; \
          done
        if: matrix.style == 'rootless'
      - name: Use podman ${{ matrix.inner-podman-version }} in the container
        run: sed -i 's%^FROM quay\.io/podman/stable.*%FROM quay.io/podman/stable:${{ matrix.inner-podman-version }}%' Dockerfile
      - name: Build image
        run: $podman build -t localhost/kind .
      - name: Create a volume
        run: $podman volume create kind-data
      - name: Network definition for rootless
        run: echo "podman_run_opts=-v $(pwd)/kind-network.yaml:/var/lib/containers/storage/networks/kind.json" >> $GITHUB_ENV
        if: matrix.style == 'rootless'
      - name: Network definition for rootful
        run: echo "podman_run_opts=-v $(pwd)/kind-network.yaml:/etc/containers/networks/kind.json" >> $GITHUB_ENV
        if: matrix.style == 'rootful'
      - name: Run the podman container
        run: $podman run -d --privileged --read-only --name kind -v kind-data:/var/lib/containers $podman_run_opts -p 6443:6443 localhost/kind
      - name: Cluster configuration for rootless
        run: echo "kind_create_opts=--config /etc/kind-cluster-rootless.yaml" >> $GITHUB_ENV
        if: matrix.style == 'rootless'
      - name: Create kind cluster
        run: $podman exec kind kind create cluster --retain $kind_create_opts
      - run: $podman exec kind podman logs kind-control-plane
        if: ${{ failure() }}
      - run: $podman exec kind kubectl cluster-info --context kind-kind
      - run: $podman exec -ti kind kubectl wait --for=condition=ready -n kube-system pod/etcd-kind-control-plane pod/kube-apiserver-kind-control-plane --timeout=60s
      - run: $podman exec kind kubectl get nodes -o jsonpath='{.items[*].metadata.name}' | xargs $podman exec kind kubectl wait --for=condition=ready node
      - run: $podman exec kind kubectl get nodes
      - run: $podman exec kind kubectl get all -A
      - run: |
          $podman exec kind curl -sk https://127.0.0.1:6443/ | tee /dev/stderr | grep -q 'forbidden: User \\"system:anonymous\\" cannot get path'
      - run: |
          curl -s --cacert <( $podman exec kind bash -c 'cat $KUBECONFIG' | awk '/certificate-authority-data:/ { print $2 }' | base64 -d ) https://127.0.0.1:6443/ | tee /dev/stderr | grep -q 'forbidden: User \\"system:anonymous\\" cannot get path'
      - run: curl -s --cacert <( $podman exec kind bash -c 'cat $KUBECONFIG' | awk '/certificate-authority-data:/ { print $2 }' | base64 -d ) -E <( $podman exec kind bash -c 'cat $KUBECONFIG' | awk '/client-certificate-data:/ { print $2 }' | base64 -d ) --key <( $podman exec kind bash -c 'cat $KUBECONFIG' | awk '/client-key-data:/ { print $2 }' | base64 -d ) https://127.0.0.1:6443/ | tee /dev/stderr | grep -Fq /apis/authentication.k8s.io/v1
      - run: $podman rm -f kind
      - run: $podman run -d --privileged --read-only --name kind -v kind-data:/var/lib/containers $podman_run_opts -p 6443:6443 localhost/kind
      - run: $podman exec -ti kind podman start --all
      - run: $podman exec -ti kind podman wait --condition=running kind-control-plane ; sleep 10
      - run: $podman exec kind kubectl get all -A

  kind-in-k8s:
    name: Kind in ${{ matrix.kubernetes }} / ${{ matrix.runtime }}
    runs-on: ubuntu-24.04${{ matrix.arch == 'arm64' && '-arm' || '' }}
    strategy:
      fail-fast: false
      matrix:
        arch: [ arm64 ]
        kubernetes: [ k3s, rke2, 'kubeadm init' ]
        runtime: [ cri-o, containerd ]
    timeout-minutes: 15
    steps:
      - uses: actions/checkout@v4
      - name: Install and setup CRI-O
        run: |
          CRIO_VERSION=v1.32
          curl -fsSL https://pkgs.k8s.io/addons:/cri-o:/stable:/$CRIO_VERSION/deb/Release.key \
              | gpg --dearmor | sudo tee /etc/apt/keyrings/cri-o-apt-keyring.gpg > /dev/null
          echo "deb [signed-by=/etc/apt/keyrings/cri-o-apt-keyring.gpg] https://pkgs.k8s.io/addons:/cri-o:/stable:/$CRIO_VERSION/deb/ /" \
              | sudo tee /etc/apt/sources.list.d/cri-o.list

          sudo apt update
          sudo apt install -y cri-o

          sudo rm -f /etc/cni/net.d/*.conflist
          sudo cp /etc/cni/net.d/10-crio-bridge.conflist.disabled /etc/cni/net.d/10-crio-bridge.conflist
          sudo systemctl start crio.service
        if: matrix.runtime == 'cri-o'
      - name: Setup the default containerd
        run: |
          sudo sed -i 's/disabled_plugins.*/### &/' /etc/containerd/config.toml
          sudo tee -a /etc/containerd/config.toml < containerd-config.toml
          sudo rm -f /etc/cni/net.d/*
          sudo cp 10-bridge.conflist /etc/cni/net.d/
          sudo systemctl restart containerd
          diff -u <( sudo containerd config default ) <( sudo containerd config dump ) || :
        if: matrix.kubernetes == 'kubeadm init' && matrix.runtime == 'containerd'
      - name: Install and setup K3s
        run: |
          export INSTALL_K3S_EXEC
          if [ -e /var/run/crio/crio.sock ] ; then
              INSTALL_K3S_EXEC='--container-runtime-endpoint /var/run/crio/crio.sock'
          fi
          curl -sfL https://get.k3s.io | sh -s - --write-kubeconfig ~/.kube/config --write-kubeconfig-group $( id -g ) --write-kubeconfig-mode 640 --disable=traefik --disable=metrics-server
          if [ -e /var/run/crio/crio.sock ] ; then
              ( echo '[crio.network]' ; echo 'plugin_dirs = [ "/var/lib/rancher/k3s/data/cni" ]' ) | sudo tee /etc/crio/crio.conf.d/20-cni.conf
              sudo systemctl restart crio.service
          fi
        if: matrix.kubernetes == 'k3s'
      - name: Install and setup RKE2
        run: |
          sudo rm -f /etc/cni/net.d/*.conflist
          sudo mkdir -p /etc/rancher/rke2/config.yaml.d
          if [ -e /var/run/crio/crio.sock ] ; then
              echo 'container-runtime-endpoint: /var/run/crio/crio.sock' | sudo tee /etc/rancher/rke2/config.yaml.d/70-crio.yaml
          fi
          curl -sfL https://get.rke2.io | sudo sh -
          sudo systemctl start rke2-server.service
          systemctl status rke2-server.service

          mkdir ~/.kube
          sudo cat /etc/rancher/rke2/rke2.yaml > ~/.kube/config
          sudo ln -s /var/lib/rancher/rke2/bin/kubectl /usr/local/bin

          kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml

          kubectl get pod -A
          echo ---
          while kubectl get pod -A --no-headers | grep -v -E 'Running|Completed' ; do echo --- ; sleep 3 ; done
          kubectl get pod -A
        if: matrix.kubernetes == 'rke2'
      - name: Install and setup Kubernetes using kubeadm init
        run: |
          KUBERNETES_VERSION=v1.33
          curl -fsSL https://pkgs.k8s.io/core:/stable:/$KUBERNETES_VERSION/deb/Release.key \
              | gpg --dearmor | sudo tee /etc/apt/keyrings/kubernetes-apt-keyring.gpg > /dev/null
          echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/$KUBERNETES_VERSION/deb/ /" \
              | sudo tee /etc/apt/sources.list.d/kubernetes.list

          sudo apt update
          sudo apt install -y kubelet kubeadm kubectl kubernetes-cni

          sudo modprobe br_netfilter
          sudo sysctl -w net.ipv4.ip_forward=1
          sudo sudo iptables -A FORWARD -o cni0 -j ACCEPT

          if [ -e /var/run/crio/crio.sock ] ; then
              CONFIG=k8s-initconfiguration-crio.yaml
          else
              CONFIG=k8s-initconfiguration-containerd.yaml
          fi
          sudo kubeadm init --config $CONFIG

          mkdir ~/.kube
          sudo cp -f /etc/kubernetes/admin.conf $HOME/.kube/config
          sudo chown $( id -u ):$( id -g ) ~/.kube/config

          kubectl taint nodes $( hostname ) node-role.kubernetes.io/control-plane:NoSchedule-
          kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml
        if: matrix.kubernetes == 'kubeadm init'

      - name: Build image
        run: docker build -t localhost/kind .
      - run: sudo skopeo copy docker-daemon:localhost/kind:latest containers-storage:localhost/kind
        if: matrix.runtime == 'cri-o'
      - run: docker save localhost/kind | sudo k3s ctr images import -
        if: matrix.kubernetes == 'k3s' && matrix.runtime == 'containerd'
      - run: docker save localhost/kind | sudo /var/lib/rancher/rke2/bin/ctr --address /run/k3s/containerd/containerd.sock -n k8s.io images import -
        if: matrix.kubernetes == 'rke2' && matrix.runtime == 'containerd'
      - run: docker save localhost/kind | sudo ctr -n k8s.io images import -
        if: matrix.kubernetes == 'kubeadm init' && matrix.runtime == 'containerd'
      - run: sudo systemctl stop docker.service docker.socket

      - name: Wait for the cluster to become ready
        run: while ! kubectl get nodes ; do sleep 5 ; done ; kubectl get nodes -o jsonpath='{.items[*].metadata.name}' | xargs kubectl wait --for=condition=ready node
      - run: kubectl get all -A
      - run: while ! kubectl get serviceaccount/default ; do sleep 5 ; done
      - run: kubectl apply -f - < kind-cluster-pod.yaml
      - run: while ! kubectl get pod/kind-cluster -o jsonpath="{.status.initContainerStatuses[0].state['running','terminated'].startedAt}" | grep . ; do kubectl get pod/kind-cluster ; sleep 5 ; done
      - run: kubectl logs -f pod/kind-cluster -c create-cluster
      - run: kubectl describe pod/kind-cluster
      - run: kubectl get pod/kind-cluster -o jsonpath="{.status.initContainerStatuses[0].state['terminated'].exitCode}" | grep ^0$
      - run: while ! kubectl get pod/kind-cluster -o jsonpath="{.status.containerStatuses[0].state['running','terminated'].startedAt}" | grep . ; do kubectl get pod/kind-cluster ; sleep 5 ; done
      - run: kubectl logs pod/kind-cluster
      - run: kubectl get pod/kind-cluster
      - run: kubectl describe pod/kind-cluster
      - run: kubectl exec pod/kind-cluster -- podman ps -a
      - run: while ! kubectl exec pod/kind-cluster -- curl -ks https://127.0.0.1:6443/ ; do sleep 1 ; done
      - run: kubectl exec pod/kind-cluster -- kubectl get nodes -o jsonpath='{.items[*].metadata.name}' | xargs kubectl exec pod/kind-cluster -- kubectl wait --for=condition=ready node
      - run: kubectl exec pod/kind-cluster -- kubectl get nodes
      - run: kubectl exec pod/kind-cluster -- kubectl get all -A

      - run: kubectl exec pod/kind-cluster -- kubectl create serviceaccount -n default admin
      - run: kubectl exec pod/kind-cluster -- kubectl patch clusterrolebinding cluster-admin --type=json -p='[{"op":"add", "path":"/subjects/-", "value":{"kind":"ServiceAccount", "namespace":"default", "name":"admin" } }]'
      - run: kubectl --kubeconfig=./kubeconfig config set-cluster kind --server=https://$( kubectl get service/kind-cluster -o jsonpath='{.spec.clusterIP}:{.spec.ports[0].port}' ) --insecure-skip-tls-verify=true
      - run: kubectl --kubeconfig=./kubeconfig config set-credentials kind-admin --token=$( kubectl exec pod/kind-cluster -- kubectl create token -n default admin )
      - run: kubectl --kubeconfig=./kubeconfig config set-context kind --cluster=kind --user=kind-admin
      - run: kubectl --kubeconfig=./kubeconfig config use-context kind
      - run: kubectl --kubeconfig=./kubeconfig get all -A

      - run: kubectl delete pod/kind-cluster
      - run: kubectl apply -f - < kind-cluster-pod.yaml
      - run: while ! kubectl get pod/kind-cluster -o jsonpath="{.status.containerStatuses[0].state['running','terminated'].startedAt}" | grep . ; do kubectl get pod/kind-cluster ; sleep 5 ; done
      - run: kubectl logs pod/kind-cluster -c create-cluster
      - run: kubectl logs pod/kind-cluster
      - run: while ! kubectl exec pod/kind-cluster -- curl -ks https://127.0.0.1:6443/ ; do sleep 1 ; done
      - run: kubectl exec pod/kind-cluster -- kubectl get all -A

      - run: kubectl --kubeconfig=./kubeconfig get all -A

      - name: Setup upterm session
        uses: lhotari/action-upterm@v1
        if: failure()
